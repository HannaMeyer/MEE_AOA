---
title: "Mapping the area of applicability - Case Study"
subtitle: "Supplementary to the paper 'Predicting into unknown space? Estimating the area of applicability of spatial prediction models'"
author: "Hanna Meyer"
date: "1/4/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
This script contains the complete code for the case study within the paper and creates the figures presented there.
It can further be used to run experiments under different settings. 
Note that running the code takes a few minutes (depending on the number of training data and the size of the study area)!

## Getting started 
Major functionality needed is the 'aoa' function from the 'CAST' package that is doing the distance based estimation of the area of applicability. 'Caret' is needed for model training. The case study uses a simulated prediction task based on the 'virtualspecies' package.

```{r libraries, message = FALSE}
rm(list=ls())
#install_github("HannaMeyer/CAST")
library(virtualspecies)
library(caret)
library(CAST)
library(viridis)
library(gridExtra)
library(knitr)
library(grid)
library(latticeExtra)

rmse <- function(pred,obs){sqrt( mean((pred - obs)^2, na.rm = TRUE) )}
```

Settings for generating the predictors and response need to be defined, as well as the number of training data points and the seed used for all functions that involve randomness.
The settings specified here are used for the case study published in the paper. Feel free to change them to see how things work under different scenarios!

### Getting started 


Note: To reproduce results used in the paper use the following settings:

* Default Case study with random data: npoints=50,design="random",meansPCA= c(3, -1), sdPCA=c(2, 2),simulateResponse=c("bio2","bio5","bio10", "bio13","bio14","bio19"),studyarea=c(-15, 65, 30, 75), seed=10

* Case Study with clustered data: npoints=500,nclusters=50,design="clustered",
maxdist <- 0.8, meansPCA= c(3, -1), sdPCA=c(2, 2), simulateResponse=c("bio2","bio5","bio10", "bio13","bio14", "bio19"), studyarea=c(-15, 65, 30, 75), seed=10


Further interesting settings:

* Case Study biased with outlier: npoints=50,design="biasedWithOutlier",countries = c("Germany","Ireland","France", "Sweden"), countriesOutlier= "Turkmenistan", meansPCA= c(3, -1), sdPCA=c(2, 2), simulateResponse=c("bio2","bio5","bio10", "bio13","bio14", "bio19"), studyarea=c(-15, 65, 30, 75), seed=10

* Case Study biased: same as biased with outlier but with design= "outlier"


```{r settings}
npoints <- 50 # number of training samples

design <- "random"  # either clustered,biased,random, biasedWithOutlier

countries <- c("Germany","Ireland","France", "Sweden") #if design==biased
countriesOutlier <- "Turkmenistan" #if design==biasedWithOutlier A single point is set here
nclusters <- 50 #number of clusters if design==clustered
maxdist <- 0.8 #maxdist for clustered samples if design==clustered
meansPCA <- c(3, -1) # means of the gaussian response functions to the 2 axes
sdPCA <- c(2, 2) # sd's of the gaussian response functions to the 2 axes
simulateResponse <- c("bio2","bio5","bio10", "bio13",
                      "bio14","bio19") # variables used to simulate the response
studyarea <- c(-15, 65, 30, 75) # extent of study area. Default: Europe
seed <- 10
```

# Get data
Bioclim data are downloaded and cropped to the study area.

```{r data, message = FALSE}
predictors_global <- raster::getData('worldclim', var='bio', res=10, path='../data/')
wp <- extent(studyarea)
predictors <- crop(predictors_global,wp)

#create a mask for land area:
mask <- predictors[[1]]
values(mask)[!is.na(values(mask))] <- 1

```


## Generate Predictors and Response
The virtual response variable is created based on the PCA of a subset of bioclim predictors. See the virtualspecies package for further information.

```{r variables,message = FALSE}
response_vs <- generateSpFromPCA(predictors[[simulateResponse]],
                                 means = meansPCA,sds = sdPCA, plot=F)

response <- response_vs$suitab.raster



```

## Simulate training points
To simulate field locations that are typically used as training data, "npoints" locations are randomly selected.
If a clustered design is used, the "npoints" are distributed over "nclusters" with a maximum distance between each point of a cluster (maxdist, in degrees).


```{r clusteredpoints,message = FALSE, include=FALSE}
#For a clustered sesign:
csample <- function(x,n,nclusters,maxdist,seed){
  set.seed(seed)
  cpoints <- sp::spsample(x, n = nclusters, type="random")
  result <- cpoints
  result$clstrID <- 1:length(cpoints)
  for (i in 1:length(cpoints)){
    ext <- rgeos::gBuffer(cpoints[i,], width = maxdist)
    newsamples <- sp::spsample(ext, n = (n-nclusters)/nclusters, 
                               type="random")
    newsamples$clstrID <- rep(i,length(newsamples))
    result <- rbind(result,newsamples)
    
  }
  result$ID <- 1:nrow(result)
  return(result)
}
```

```{r samplepoints, message = FALSE, warning=FALSE, include=FALSE}
mask <- rasterToPolygons(mask,dissolve=TRUE)
set.seed(seed)
if (design=="clustered"){
  samplepoints <- csample(mask,npoints,nclusters,maxdist=maxdist,seed=seed)
} 
if (design=="biased"){
  countryboundaries <- getData("countries", path='../data/')
  countryboundaries <- countryboundaries[countryboundaries$NAME_ENGLISH%in%c(countries),]
  samplepoints <- spsample(countryboundaries,npoints,"random")
}
if (design=="biasedWithOutlier"){
  countryboundaries <- getData("countries", path='../data/')
  countryboundariesOut <- countryboundaries[countryboundaries$NAME_ENGLISH%in%c(countriesOutlier),]
  countryboundaries <- countryboundaries[countryboundaries$NAME_ENGLISH%in%c(countries),]
  samplepoints <- spsample(countryboundaries,npoints,"random")
  samplepoints <- rbind(samplepoints,spsample(countryboundariesOut,1,"random"))
}  

if (design=="random"){
  samplepoints <- spsample(mask,npoints,"random")
}


```


```{r vis_data, messages = FALSE, echo = FALSE, include=FALSE}

png("../figures/trainingdata.png", width=26, height=10,units = "cm",res=300)
p1 <- spplot(stretch(predictors[[simulateResponse]],0,1),col.regions=viridis(100),
             par.settings =list(strip.background=list(col="grey")))

p2 <-spplot(response,col.regions=viridis(100),
            sp.layout=list("sp.points", samplepoints, col = "red", first = FALSE))
grid.arrange(p1,p2,ncol=2,nrow=1,widths=c(1.25,1))

grid.text("a",x = unit(0.02, "npc"), 
          y = unit(0.96, "npc"),
          just = "left")
grid.text("b",x = unit(0.57, "npc"), 
          y = unit(0.96, "npc"),
          just = "left")
invisible(dev.off())

```

```{r figs1, echo=FALSE,out.width="100%",fig.cap="Subset of the predictors as well as the response variable and the selected training data points"}
include_graphics("../figures/trainingdata.png")
```


# Model training and prediction

## Preparation
To prepare model training, predictor variables are extracted for the location of the selected sample data locations.

```{r traindat}
trainDat <- extract(predictors,samplepoints,df=TRUE)
trainDat$response <- extract (response,samplepoints)

if (design=="clustered"){
  trainDat <- merge(trainDat,samplepoints,by.x="ID",by.y="ID")
}

trainDat <- trainDat[complete.cases(trainDat),]
```

#### Visualize Response
```{r plot_trdat}

#pca_pred <- predict(response_vs$details$pca,trainDat[,response_vs$details$variables])

#axes <- response_vs$details$axes
#axes.to.plot <- axes[1:2]
# xmin <- min(response_vs$details$pca$li[, axes.to.plot[1]]) - 0.3 * diff(range(response_vs$details$pca$li[, axes.to.plot[1]]))
# xmax <- max(response_vs$details$pca$li[, axes.to.plot[1]])
# 
# pdf("../figures/plotresponse.pdf",width=6,height=6)
# plotResponse(response_vs,no.plot.reset=T)
# points(((pca_pred[,1]+10)/21.5),
#        pca_pred[,2],pch=16)
# dev.off()
```

```{r figs_plotres, echo=FALSE,out.width="100%",fig.cap="Subset of the predictors as well as the response variable and the selected training data points"}
#include_graphics("../figures/plotresponse.pdf")
```


## Training and cross-validation
Model training is then done using the caret package. Note that other packages work as well as long as variable importance can be derived. The model output gives information on the general estimated model performance based on random cross validation.

```{r training}
set.seed(seed)
if(design!="clustered"){
  model <- train(trainDat[,names(predictors)],
                 trainDat$response,
                 method="rf",
                 importance=TRUE,
                 tuneGrid = expand.grid(mtry = c(2:length(names(predictors)))),
                 trControl = trainControl(method="cv",savePredictions = TRUE))
  
  
  print("random cross-validation performance:")
  print(model)
}
#if data are clustered, clustered CV is used:
if(design=="clustered"){
  folds <- CreateSpacetimeFolds(trainDat, spacevar="clstrID",k=nclusters)
  model <- train(trainDat[,names(predictors)],
                 trainDat$response,
                 method="rf",
                 importance=TRUE,
                 tuneGrid = expand.grid(mtry = c(2:length(names(predictors)))),
                 trControl = trainControl(method="cv",index=folds$index,savePredictions = TRUE))
  print("leave-cluster-out cross-validation performance:")
  print(model)
  
  model_random <- train(trainDat[,names(predictors)],
                        trainDat$response,
                        method="rf",
                        importance=TRUE,
                        tuneGrid = expand.grid(mtry = c(2:length(names(predictors)))),
                        trControl = trainControl(method="cv",savePredictions = TRUE))
  
  print("random cross-validation performance:")
  print(model_random)
}
```


## Prediction and error calculation

The trained model is used to make predictions for the entire study area. The absolute error between prediction and reference is calculated for later comparison with the dissimilarity index.

```{r predict}
prediction <- predict(predictors,model)
truediff <- abs(prediction-response)
```

# Estimating the area of applicability

## Variable importance to get weights
The variable importance from model training can be visualized to get information on how variable weighting will be done during to estimation of the dissimilarity index

```{r varimp,message=FALSE, echo=FALSE}
cairo_pdf("../figures/varimp.pdf", width=4, height=3.5)
plot(varImp(model,scale = F),col="black")
invisible(dev.off())
```


```{r figs3, echo=FALSE, out.width="75%",fig.cap="Variable importance"}
include_graphics("../figures/varimp.pdf")
```

The area of applicability and the dissimilarity index are then calculated. First using weighted variables, and second (for comparison) without weighting. Everything is run in parallel to speed things up.

```{r uncert,message=FALSE}

#with variable weighting:
AOA <- aoa(predictors,model=model)
#without weighting:
AOA_noWeights <- aoa(predictors,train=trainDat,variables = names(predictors))

if (design=="clustered"){
  AOA_random<- aoa(predictors, model_random)
}

preds <- model$pred[model$pred$mtry==model$bestTune$mtry,]
absError <- abs(preds$pred[order(preds$rowIndex)]-preds$obs[order(preds$rowIndex)])


predtrain <- predict(model$finalModel,trainDat,predict.all=TRUE) 
sdpred <-  apply(predtrain$individual,1,sd)
```

```{r uncert_weights,message=FALSE,include=FALSE}

png("../figures/caseStudy_DIweight.png",width=24, height=9,units = "cm",res=300)
spplot(stack(AOA_noWeights$DI,AOA$DI),col.regions=viridis(100),
  names.attr=c('DI (unweighted)',"DI (weighted)"),
   par.settings =list(strip.background=list(col="grey")))
invisible(dev.off())

```


```{r, echo=FALSE, out.width="100%",fig.cap="Comparison between the DI using either weighted (a) or unweighted (b) distances in the predictor space"}
include_graphics("../figures/caseStudy_DIweight.png")
```


# Standard deviation from individual trees for comparison

For camparison to what is often used as uncertainty information, the standard deviations of the individual predictions from the 500 developed trees within the Random Forest model are calculated.

```{r RFsd, echo=FALSE}
RFsd <- function(predictors,model){
  prep <- as.data.frame(predictors)
  prep[is.na(prep)] <- -9999
  pred_all <- predict(model$finalModel,prep,predict.all=TRUE)
  sds <-  apply(pred_all$individual,1,sd)
  predsd <- predictors[[1]]
  values(predsd) <- sds
  values(predsd)[prep[,1]==-9999] <- NA
  return(predsd)
}
```


```{r calcRFsd}
predsd <- RFsd(predictors,model)

```





## Comparison between prediction, error, DI and cross validation
```{r lm_uncert,echo=FALSE}

#DI with weights:
paste0("correlation coefficient (true error~DI)= ",
       cor(values(truediff),values(AOA$DI),use="complete.obs"))
#DI no weights:
paste0("correlation coefficient (true error~DI estimated without weights)= ",
       cor(values(truediff),values(AOA_noWeights$DI),use="complete.obs"))

#comparison prediction~ref
paste0("R² (prediction~reference)= ",
       summary(lm(values(response)~values(prediction)))$r.squared)

paste0("r (prediction~reference)= ",
       cor(values(response),values(prediction),use = "complete.obs"))

paste0("RMSE (prediction,reference)= ",
       rmse(values(response),values(prediction)))

#comparison prediction for the AOA~ref
predictionAOI <- prediction
values(predictionAOI)[values(AOA$AOA)==0] <- NA
paste0("R^2 AOA= ",summary(lm(values(response)~values(predictionAOI)))$r.squared)
paste0("r AOA= ",cor(values(response),values(predictionAOI),use="complete.obs"))
paste0("RMSE AOA= ",rmse(values(response),values(predictionAOI)))

# ...and for outside the AOA
predictionNOTAOI <- prediction
values(predictionNOTAOI)[values(AOA$AOA)==1] <- NA
paste0("R^2 outside AOA= ",summary(lm(values(response)~values(predictionNOTAOI)))$r.squared)
paste0("r outside AOA= ",cor(values(response),values(predictionNOTAOI),use="complete.obs"))
paste0("RMSE outside AOA= ",rmse(values(response),values(predictionNOTAOI)))

```

## Standard deviations of predictions within the AOA

The standard deviations of an ensemble are not helpful outside the AOA, however, inside the AOA they can provide helpful indicators of uncertainty, e.g. due to low data point densities.

```{r lm_SDwithinAOA, include=FALSE}
png("../figures/caseStudy_sdWithinAoa.png",width=10, height=10,units = "cm",res=300)
spplot(predsd,col.regions=viridis(100))+spplot(AOA$AOA,col.regions=c("violetred","transparent"))
invisible(dev.off())
```


```{r figs_sd, echo=FALSE,out.width="100%",fig.cap="Standard deviations of the random forest ensemble within the AOA"}
include_graphics("../figures/caseStudy_sdWithinAoa.png")
```


# Comparison

The dissimilarity index, as well as the standard deviations can then be compared to the true error. 

```{r prepare_comp}
compare <- stack(response,prediction,
                 predsd,truediff, 
                 AOA$DI)
names(compare) <- c("response","prediction", "sd","true_diff","DI")
summary(values(compare))
```

```{r plot_comp, message=FALSE, echo=FALSE}
p <- list()
txts <- c("a","b","c","d","e","f")
for (i in 1:nlayers(compare)){
  cols <-viridis(100)
  txt <- list("sp.text", c(-12,72), txts[i])
  zlims <- NULL
  if(names(compare)[i]%in%c("response","prediction")){
    lims <- seq(min(values(compare[[c("response","prediction")]]),na.rm=TRUE),
                max(values(compare[[c("response","prediction")]]),na.rm=TRUE),
                (max(values(compare[[c("response","prediction")]]),na.rm=TRUE)-
                   min(values(compare[[c("response","prediction")]]),na.rm=TRUE))/100)
  } else{
    lims <- seq(min(values(compare[[i]]),na.rm=TRUE),
                max(values(compare[[i]]),na.rm=TRUE),
                (max(values(compare[[i]]),na.rm=TRUE)-
                   min(values(compare[[i]]),na.rm=TRUE))/100)
    
  }
  p[[i]] <- spplot(compare[[i]],col.regions = cols, 
                   sp.layout=list(txt),at=lims,pretty=TRUE)
}

lims <- seq(min(values(compare[[c("response","prediction")]]),na.rm=TRUE),
            max(values(compare[[c("response","prediction")]]),na.rm=TRUE),
            (max(values(compare[[c("response","prediction")]]),na.rm=TRUE)-
               min(values(compare[[c("response","prediction")]]),na.rm=TRUE))/100)
p[[i+1]] <- sp::spplot(prediction, col.regions=viridis(100),at=lims,pretty=TRUE,
                       sp.layout=list(list("sp.text", c(-12,72), txts[i+1])))+sp::spplot(AOA$AOA,col.regions=c("violetred","transparent"))


png("../figures/caseStudy_PredRef.png",width=24, height=14,units = "cm",res=350)
grid.arrange(p[[1]],p[[2]], p[[3]],
             p[[4]],p[[5]],p[[6]],
             nrow=2,ncol=3)
invisible(dev.off())
```

```{r figs4, echo=FALSE,out.width = "100%", fig.cap="Comparison between reference (a), prediction (b), standard deviation of predictions (c), the true error (d), DI based on weighted variables (e), masked predictions (f)"}

include_graphics("../figures/caseStudy_PredRef.png")
```





## If applicable: Comparison random vs spatial CV and AOA estimation

```{r lm_uncert_sp, echo=FALSE}
if(design!="clustered"){
  print("not applicable here because no clustered design was chosen") 
}
if(design=="clustered"){
  prediction_random <- predict(predictors,model_random)
  # results for random CV model:
  predictionAOI <- prediction_random
  values(predictionAOI)[values(AOA_random$AOA)==0] <- NA
  print(paste0("R^2 AOA= ",summary(lm(values(response)~values(predictionAOI)))$r.squared))
  print(paste0("r AOA= ",cor(values(response),values(predictionAOI),use="complete.obs")))
  
  print(paste0("RMSE AOA= ", rmse(values(response),values(predictionAOI))))
  
  predictionNOTAOI <- prediction_random
  values(predictionNOTAOI)[values(AOA_random$AOA)==1] <- NA
  print(paste0("R^2 outside AOA= ",summary(lm(values(response)~values(predictionNOTAOI)))$r.squared))
  print(paste0("r outside AOA= ",cor(values(response),values(predictionNOTAOI),use="complete.obs")))
  print(paste0("RMSE outside AOA= ",rmse(values(response),values(predictionNOTAOI))))
}
```

```{r compare_spatial,echo=FALSE}
if (design=="clustered"){
  p <- list()
  p[[1]] <-  spplot(AOA$DI,col.regions = cols,
                    sp.layout=list(list("sp.points", samplepoints, col = "red", first = FALSE),list("sp.text", c(-12,72), "a")))
  p[[2]] <- sp::spplot(prediction, col.regions=viridis(100),sp.layout=list(list("sp.text", c(-12,72), "b")))+sp::spplot(AOA$AOA,col.regions=c("violetred","transparent"))
  
  p[[3]] <- sp::spplot(prediction_random, col.regions=viridis(100),sp.layout=list(list("sp.text", c(-12,72), "c")))+sp::spplot(AOA_random$AOA,col.regions=c("violetred","transparent"))
  
  png("../figures/caseStudy_SPvsRandom.png",width=24, height=9,units = "cm",res=300)
  
  grid.arrange(p[[1]],p[[2]], p[[3]],
               nrow=1,ncol=3)
  invisible(dev.off())
}
```

```{r figs6, echo=FALSE, out.width="100%",fig.cap="Comparison between spatial vs non spatial estimation of the AOA"}
if(design=="clustered"){
  include_graphics("../figures/caseStudy_SPvsRandom.png")
}
```

## DI as a quantitative uncertainty measure?

### Relationship between the DI and the absolute error on a data-point-level

Plotting the RMSE of cross-validated prodictions as well as the corresponding DI gives an impression about the error~DI relationship.
This can also be visualized via hexbins for the true error~DI relationship.



```{r hexbin, warning=FALSE, message=FALSE, echo=FALSE}
dat_all <- data.frame("true_abs_error"=values(truediff),                    "type"=rep("DI",length(values(AOA$DI))), 
                      "value"=values(AOA$DI))
th <- AOA$parameters$threshold

dat2 <- data.frame("DI"=AOA$parameters$trainDI,
                   "error"=absError)


png("../figures/hexbin.png", width=11, height=10,units="cm",res=300)
ggplot(dat_all, aes(value,true_abs_error)) +
  stat_binhex(bins=100)+
  ylab("True absolute error")+
  xlab("Dissimilarity Index")+
  geom_vline(xintercept=th,linetype="dashed",col="red")+
  scale_fill_gradientn(name = "data points",
                       trans = "log",
                       breaks = 10^(0:3),
                       colors=viridis(10, alpha=.7))+
  theme_bw()+
  theme(legend.title = element_text( size = 10))+
  geom_point(data=dat2,aes(DI,error))#+
#stat_smooth(data=dat2,aes(DI,error),method="lm",se = FALSE,col="red")

invisible(dev.off())
```



```{r figs5, echo=FALSE, out.width="50%",fig.cap="Relationship between the DI and the true error for the cross-validated training data (black points) as well as for the entire spatial reference dataset (hexbins)"}
include_graphics("../figures/hexbin.png")
```


### Relationship between the DI and the requested performance measure (here RMSE) 

The relationship between error and DI can be used to limit predictioons to an area (within the AOA) where a required performance (e.g. RMSE, R2, Kappa, Accuracy) applies.
This can be done using the result of calibrate_aoa which used the relationship analyzed in a window of DI values. The corresponding model (here: shape constrained additive models which is the default: Monotone increasing P-splines with the dimension of the basis used to represent the smooth term is 6 and a 2nd order penalty.) can be used to estimate the performance on a pixel level, which then allows limiting predictions using a threshold.

```{r userAOA_cv, warning=FALSE, message=FALSE, echo=FALSE,include=FALSE}

#if(nrow(model$trainingData>=250)){
#  window.size <- 10 # clustered has more data so a larger window size is possible
#}else{
#  window.size <- 5
#}
window.size <- 10

AOA_new <- calibrate_aoa(AOA,model,window.size = window.size,showPlot=FALSE)
recl <- attributes(AOA_new$AOA)$calib$group_stats

reference_perf <- as.data.frame(stack(AOA$DI,response,prediction))
reference_perf <- reference_perf[order(reference_perf$DI),]
names(reference_perf) <- c("DI","obs","pred")


### do this for ul,ll of training data
slidingw <- attributes(AOA_new$AOA)$calib$group_stats
reference_metric <- apply(slidingw,1,function(x){
  x_df <- data.frame(t(x))
  subs_ref <- reference_perf[reference_perf$DI>x_df$ll&
                               reference_perf$DI<x_df$ul,]
  rmse(subs_ref[,"pred"],subs_ref[,"obs"])
})


```


#### Using muliple CV's

The calibration shown above uses the cross-validion folds used during model training only. However, to be able to test the ability of the model to do both, interpolation and extrapolation, we suggest to test multiple cross-validations. Here, we split the data into folds by clusterin in the predictor space. We use all cross-validated predictions and DI values to then estimate the error~Di relationship.
Since cross-validation is done with respect to extrapolation as well, the estimated AOA is in most cases larger compared to the application of a CV which is not explicitely testing for extrapolation.

We compare the relationship estimated by cross-validation with the true relationship from the reference.


```{r userAOA_multicv, warning=FALSE, message=FALSE, echo=FALSE,include=FALSE}


AOA_new_MCV <- calibrate_aoa(AOA,model,multiCV=TRUE,length.out = 10,
                             window.size = window.size,showPlot=FALSE)

recl_mcv <- attributes(AOA_new_MCV$AOA)$calib$group_stats

reference_perf_MCV <- as.data.frame(stack(AOA$DI,response,prediction))
reference_perf_MCV <- reference_perf_MCV[order(reference_perf_MCV$DI),]
names(reference_perf_MCV) <- c("DI","obs","pred")

slidingw <- attributes(AOA_new_MCV$AOA)$calib$group_stats
reference_metric_MCV <- apply(slidingw,1,function(x){
  x_df <- data.frame(t(x))
  subs_ref <- reference_perf_MCV[reference_perf_MCV$DI>x_df$ll&
                                   reference_perf_MCV$DI<x_df$ul,]
  rmse(subs_ref[,"pred"],subs_ref[,"obs"])
})



cairo_pdf(paste0("../figures/AOA_calibrate_",design,".pdf"), width=10, height=4.5)
par(mar=c(4, 4, 0.2, 0.2), mfrow=c(1,2))

plot(recl$DI,recl$RMSE,xlab="DI",ylab=model$metric,xlim=c(min(recl$DI,recl_mcv$DI),max(recl$DI,recl_mcv$DI)),ylim=c(min(recl$RMSE,recl_mcv$RMSE,na.rm=T),max(recl$RMSE,recl_mcv$RMSE,na.rm=T)))
points(recl$DI,reference_metric,col="red")
legend("topright",lty=c(NA,NA,2),lwd=c(NA,NA,2),pch=c(1,1,NA),col=c("black","red","blue"),
       legend=c("CV","Truth","model"),bty="n")
lines(seq(0,max(recl$DI),0.01),
      predict(attributes(AOA_new$AOA)$calib$model,data.frame("DI"=seq(0, max(recl$DI),0.01))),lwd=2,lty=2,col="blue")
legend("topleft", "(a)", bty="n") 



plot(recl_mcv$DI,recl_mcv$RMSE,xlab="DI",
     xlim=c(min(recl$DI,recl_mcv$DI),max(recl$DI,recl_mcv$DI)),ylab="",
     ylim=c(min(recl$RMSE,recl_mcv$RMSE,na.rm=T),max(recl$RMSE,recl_mcv$RMSE,na.rm=T)),yaxt="n")

points(recl_mcv$DI,reference_metric_MCV,col="red")

lines(seq(0,max(recl_mcv$DI),0.01),
      predict(attributes(AOA_new_MCV$AOA)$calib$model,data.frame("DI"=seq(0, max(recl_mcv$DI),0.01))),
      col="blue",lty=2,lwd=2)
legend("topleft", "(b)", bty="n") 

invisible(dev.off())


stck <- stack(AOA_new$AOA$expected_RMSE,
              AOA_new_MCV$AOA$expected_RMSE)

tmp <- AOA_new$AOA$AOA
values(tmp)<- NA
ovl <- stack(AOA_new$AOA$AOA,AOA_new_MCV$AOA$AOA)

png(paste0("../figures/errormap_calib_",design,".png"),width=18, height=9,units = "cm",res=300)
spplot(stck,col.regions = cols,,
       names.attr=c('Estimated RMSE (single CV)',"Estimated RMSE (multiple CV)"),
       par.settings =list(strip.background=list(col="grey")))+
  spplot(ovl,col.regions=c("violetred","transparent"))

invisible(dev.off())
```


```{r}
# model using single-CV
print(paste0("maximum DI included in the AOA: ",max(recl$DI,na.rm=T)))
print(summary(attributes(AOA_new$AOA)$calib$model))

# model using multiple-CV
print(paste0("maximum DI included in the AOA: ",max(recl_mcv$DI,na.rm=T)))
print(summary(attributes(AOA_new_MCV$AOA)$calib$model))
```

```{r figs8, echo=FALSE, out.width="100%",fig.cap="Relationship between the cross-validated DI and the RMSE using the CV strategy used during initial model training (a) as well as multiple cross-validation folds (clusters in predictor space from few to many clusters) (b). The RMSE was estimated using predictions and observed values within a window of DI values."}
include_graphics(paste0("../figures/AOA_calibrate_",design,".pdf"))
```


```{r figs9, echo=FALSE, out.width="100%",fig.cap="Expected performance using either single CV for calibration (b) or multiple CV (c) which can be used to limit predictions to areas where a required performance applies. For comparison, the true prediction error is shown (a)."}
include_graphics(paste0("../figures/errormap_calib_",design,".png"))
```



